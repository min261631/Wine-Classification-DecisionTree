# -*- coding: utf-8 -*-
"""ID_21930306.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-bT_m6RCYDs2irHOMcbaQjX0k32TXdS

**TASK 3**

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import sklearn
import matplotlib.pyplot as plt

from sklearn.datasets import load_wine

from sklearn.model_selection import train_test_split

"""### Load the Dataset
The wine dataset is loaded using scikit-learn.
"""

data = load_wine()

"""### Prepare the Dataset
The dataset is converted into a pandas Data Frame.

wine_data.head() displays the first few rows of the DataFrame to give a quick preview of the data structure.
"""

wine_data = pd.DataFrame(data.data, columns=data.feature_names)
wine_data['target'] = data.target
wine_data.head()

"""### Splitting the Dataset
In the code, train_test_split is used to randomly divide the dataset into training and testing sets. The test_size=0.2 parameter specifies that 20% of the data should be allocated for testing, while the remaining 80% is used for training the model.
"""

# Split the data into training and testing sets with 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(
    wine_data[data.feature_names], wine_data['target'], test_size=0.2, random_state=0)

"""### Model Training
A Decision Tree Classifier is initialized with the criterion set to 'entropy' to ensure that the tree should use entropy (information gain) to measure the quality of a split.
"""

# Importing the model to use

# Create the decision tree classifier
clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)

"""clf.fit(X_train, y_train) trains the decision tree classifier on the training data. During training, the classifier learns decision rules from the training data that can be used to classify new, unseen data.

"""

# Train the classifier
clf.fit(X_train, y_train)

"""### Finding optimal max depth
In the code, train_test_split is used to randomly divide the dataset into training and testing sets. The test_size=0.2 parameter specifies that 20% of the data should be allocated for testing, while the remaining 80% is used for training the model.
"""

# List of values to try for max_depth:
max_depth_range = list(range(1, 6))

# List to store the accuracy for each value of max_depth:
accuracy = []

# Loop over the range of max_depth values
for depth in max_depth_range:
    # Initialize the DecisionTreeClassifier with the current max_depth
    clf = DecisionTreeClassifier(max_depth=depth, random_state=0)

    # Fit the model on the training data
    clf.fit(X_train, y_train)

    # Evaluate the model on the test data and store the accuracy
    score = clf.score(X_test, y_test)
    accuracy.append(score)

# Create a figure and axis for plotting
fig, ax = plt.subplots(figsize=(10, 7))

# Plot the accuracy against the max_depth range
ax.plot(max_depth_range, accuracy)

# Set the x and y axis limits
ax.set_xlim([1, 5])
ax.set_ylim([.50, 1.00])

# Enable the grid with specific style
ax.grid(True, axis='both', linestyle=':', color='k')

# Set the labels for x and y axes
# ax.set_xticks(max_depth_range)
ax.set_xlabel('max_depth', fontsize=24)
ax.set_ylabel('Accuracy', fontsize=24)

# Adjust layout to fit everything nicely
fig.tight_layout()

# Display the plot
plt.show()

# Find the maximum accuracy and the corresponding max_depth
max_accuracy = max(accuracy)
optimal_depth = max_depth_range[accuracy.index(max_accuracy)]

print(
    f'The optimal max_depth is {optimal_depth} with an accuracy of {max_accuracy:.2f}')

"""### Predicting the labels of new data
The model uses the information learned during the model training process to predict the test set results for the new data.

Another index can be used to replace the 0 in X_test.iloc[0] to predict the class for a different data point in the testing set.
"""

prediction = clf.predict(X_test.iloc[0].values.reshape(1, -1))
print("Predicted value:", prediction[0])

"""
If the X_test.iloc[0] is removed, we can obtain an array of all the predicted values for all the data points in X_test at once."""

y_pred = clf.predict(X_test)
print("Predictions:", y_pred)

"""A dictionary lookup has been used to add meaningful values to the predicted classes."""

# Define a dictionary to map class labels to species names
# Defining the dictionary
class_names = {0: "class_0", 1: "class_1", 2: "class_2"}

# Assigning string values based on the predicted class labels
predicted_classes = class_names[prediction[0]]

# Printing the predicted class labels
print("Predicted Classes:", predicted_classes)

"""### Decision Tree Graph Visualisation
In the code, train_test_split is used to randomly divide the dataset into training and testing sets. The test_size=0.2 parameter specifies that 20% of the data should be allocated for testing, while the remaining 80% is used for training the model.
"""


fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(4, 4), dpi=300)
tree.plot_tree(clf, feature_names=data.feature_names,
               class_names=data.target_names, filled=True)
plt.savefig('output_graph.pdf')

"""### Measuring Model Accuracy
Accuracy is the number of correct predictions divided by the total number of data points
"""

# Accuracy code
score = clf.score(X_test, y_test)
print(f'Classification Accuracy: {score * 100:.2f}%')
